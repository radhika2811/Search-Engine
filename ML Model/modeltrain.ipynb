{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712b2314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model components: sentence-transformers/all-MiniLM-L6-v2\n",
      "Downloading and preparing the SNLI dataset...\n",
      "âœ… Dataset ready. Created 183416 training examples.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     39\u001b[39m warmup_steps = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_examples) / batch_size * num_epochs * \u001b[32m0.1\u001b[39m)\n\u001b[32m     41\u001b[39m args = SentenceTransformerTrainingArguments(\n\u001b[32m     42\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     num_train_epochs=num_epochs,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     logging_steps=\u001b[32m100\u001b[39m,\n\u001b[32m     51\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m trainer = \u001b[43mSentenceTransformerTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# --- Step 6: Run the Fine-Tuning ---\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸš€ Starting the fine-tuning process...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\trainer.py:318\u001b[39m, in \u001b[36mSentenceTransformerTrainer.__init__\u001b[39m\u001b[34m(self, model, args, train_dataset, eval_dataset, loss, evaluator, data_collator, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluator = evaluator\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouter_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrouter_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    320\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    322\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval_dataset = \u001b[38;5;28mself\u001b[39m.preprocess_dataset(\n\u001b[32m    323\u001b[39m         eval_dataset, prompts=args.prompts, router_mapping=args.router_mapping, dataset_name=\u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\trainer.py:1064\u001b[39m, in \u001b[36mSentenceTransformerTrainer.preprocess_dataset\u001b[39m\u001b[34m(self, dataset, prompts, router_mapping, dataset_name)\u001b[39m\n\u001b[32m   1060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# Maybe add dataset names to the dataset, useful for 1) training with prompts, 2) training with multiple losses,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# and 3) training with a router mapping.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmaybe_add_dataset_name_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouter_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[38;5;66;03m# Add a tag to the dataset to indicate that it has been preprocessed, to ensure that we don't apply the map or\u001b[39;00m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# transform multiple times.\u001b[39;00m\n\u001b[32m   1068\u001b[39m dataset._sentence_transformers_preprocessed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\trainer.py:1095\u001b[39m, in \u001b[36mSentenceTransformerTrainer.maybe_add_dataset_name_column\u001b[39m\u001b[34m(self, dataset, prompts, router_mapping, dataset_name)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1080\u001b[39m \u001b[33;03mMaybe add a dataset name column to the dataset, if\u001b[39;00m\n\u001b[32m   1081\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1092\u001b[39m \u001b[33;03m    DatasetDict | Dataset | None: The dataset with prompts or dataset names added.\u001b[39;00m\n\u001b[32m   1093\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# Ensure that there's no \"dataset_name\"/\"return_loss\" columns in the unprocessed datasets\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_column_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, (Dataset, IterableDataset)):\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sentence_transformers\\trainer.py:617\u001b[39m, in \u001b[36mSentenceTransformerTrainer.validate_column_names\u001b[39m\u001b[34m(self, dataset, dataset_name)\u001b[39m\n\u001b[32m    614\u001b[39m         \u001b[38;5;28mself\u001b[39m.validate_column_names(dataset, dataset_name=dataset_name)\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overlap := \u001b[38;5;28mset\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m) & {\u001b[33m\"\u001b[39m\u001b[33mreturn_loss\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    619\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following column names are invalid in your \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mdataset_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(overlap)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    620\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Avoid using these column names, as they are reserved for internal use.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    621\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'column_names'"
     ]
    }
   ],
   "source": [
    "# --- Step 1: All Necessary Imports ---\n",
    "import torch\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    InputExample,\n",
    "    losses,\n",
    "    models,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Step 2: Load the Pre-trained Model ---\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "print(f\"Loading pre-trained model components: {model_name}\")\n",
    "\n",
    "word_embedding_model = models.Transformer(model_name)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# --- Step 3: Prepare the Dataset ---\n",
    "print(\"Downloading and preparing the SNLI dataset...\")\n",
    "train_dataset = load_dataset('snli', split='train')\n",
    "\n",
    "train_examples = []\n",
    "for record in train_dataset:\n",
    "    if record['label'] == 0:  # 0 = entailment\n",
    "        train_examples.append(InputExample(texts=[record['premise'], record['hypothesis']]))\n",
    "\n",
    "print(f\"âœ… Dataset ready. Created {len(train_examples)} training examples.\")\n",
    "\n",
    "# --- Step 4: Define the Loss Function ---\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "# --- Step 5: Training Setup ---\n",
    "num_epochs = 1\n",
    "batch_size = 16\n",
    "warmup_steps = int(len(train_examples) / batch_size * num_epochs * 0.1)\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,   # mixed precision (faster on GPUs with AMP support)\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_examples,\n",
    "    loss=train_loss,\n",
    ")\n",
    "\n",
    "# --- Step 6: Run the Fine-Tuning ---\n",
    "print(\"\\nðŸš€ Starting the fine-tuning process...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Fine-tuning complete!\")\n",
    "print(\"Your new, improved model has been saved to the './output' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
